{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb35176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from pickle import dump, load\n",
    "\n",
    "from training_functions import summarize_data, get_encoded_labels_overshoot_undershoot_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9b2aa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         N1         D2         D1  RiseTime  TransientTime  SettlingTime  \\\n",
      "0 -2.098997   9.355879  12.169613  1.418254       2.808480      3.004993   \n",
      "1 -8.064675   7.424314  10.984939  1.178159       2.236274      2.261321   \n",
      "2  3.355627  10.362653  17.972740  0.690566       1.434136      1.434136   \n",
      "3 -5.016306   6.777631  10.215730  1.109049       2.153838      2.198836   \n",
      "4 -6.944586   7.414111  13.427289  0.904405       1.722170      1.748116   \n",
      "\n",
      "   SettlingMin  SettlingMax  Overshoot  Undershoot      Peak  PeakTime  \n",
      "0    -0.172400    -0.155525        0.0   35.908827  0.172400  5.423694  \n",
      "1    -0.733804    -0.660804        0.0    5.243774  0.733804  4.088517  \n",
      "2     0.168055     0.186666        0.0   -0.000000  0.186666  3.487674  \n",
      "3    -0.490662    -0.442658        0.0   10.675850  0.490662  3.643252  \n",
      "4    -0.516743    -0.465671        0.0    8.085863  0.516743  2.766764  \n",
      "Models with overshoot: 3334/33.339999999999996%\n",
      "Models with undershoot: 3334/33.339999999999996%\n",
      "Models with neither: 3332/33.32%\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "dataset = pd.read_csv(\"../Datasets/Three Coefficient 2nd Order/Balanced Dataset 1.csv\")\n",
    "print(dataset.head())\n",
    "summarize_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86d886b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Determine the appropriate label for each entry using one-hot encoding\n",
    "# [0 0 1] for undershoot, [0 1 0] for overshoot, [1 0 0] for neither\n",
    "labels = get_encoded_labels_overshoot_undershoot_classification(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b6e7227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the coefficient/input matrix\n",
      "(10000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy array\n",
    "data_array = dataset.to_numpy()\n",
    "\n",
    "# Separate the coefficients, which form the inputs to the neural network, from the rest of the data\n",
    "# The coefficients are the first columns in the CSV file\n",
    "coefficients = data_array[:,0:3]\n",
    "# Use StandardScaler to remove the mean and scale to unit variance\n",
    "scaler = preprocessing.StandardScaler().fit(coefficients)\n",
    "scaled_coefficients = scaler.transform(coefficients)\n",
    "\n",
    "print(\"Dimensions of the coefficient/input matrix\")\n",
    "print(scaled_coefficients.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd7884db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 3) (9000, 3) (1000, 3) (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Reserve 10% of the data as test data, and use the rest as training data\n",
    "train_inputs,test_inputs,train_labels,test_labels = model_selection.train_test_split(scaled_coefficients, labels, test_size=0.10)\n",
    "print(train_inputs.shape, train_labels.shape,test_inputs.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2698cd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer_1 (Dense)             (None, 128)               512       \n",
      "                                                                 \n",
      " layer_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,411\n",
      "Trainable params: 17,411\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = keras.models.Sequential()\n",
    "# Hidden layers each use 128 nodes with the RELU activation function\n",
    "model.add(keras.layers.Dense(128, input_shape=(3,), name='layer_1', activation='relu'))\n",
    "model.add(keras.layers.Dense(128, name='layer_2', activation='relu'))\n",
    "# Output layer uses the softmax activation function\n",
    "model.add(keras.layers.Dense(3, name='output_layer', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0777cc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.2663 - accuracy: 0.9126 - val_loss: 0.1505 - val_accuracy: 0.9439\n",
      "Epoch 2/10\n",
      "450/450 [==============================] - 1s 1ms/step - loss: 0.1222 - accuracy: 0.9551 - val_loss: 0.1115 - val_accuracy: 0.9606\n",
      "Epoch 3/10\n",
      "450/450 [==============================] - 1s 1ms/step - loss: 0.1006 - accuracy: 0.9610 - val_loss: 0.0818 - val_accuracy: 0.9650\n",
      "Epoch 4/10\n",
      "450/450 [==============================] - 1s 1ms/step - loss: 0.0887 - accuracy: 0.9650 - val_loss: 0.0786 - val_accuracy: 0.9672\n",
      "Epoch 5/10\n",
      "450/450 [==============================] - 1s 1ms/step - loss: 0.0819 - accuracy: 0.9661 - val_loss: 0.0801 - val_accuracy: 0.9678\n",
      "Epoch 6/10\n",
      "450/450 [==============================] - 1s 1ms/step - loss: 0.0785 - accuracy: 0.9688 - val_loss: 0.0796 - val_accuracy: 0.9700\n",
      "Epoch 7/10\n",
      "450/450 [==============================] - 1s 1ms/step - loss: 0.0740 - accuracy: 0.9696 - val_loss: 0.0687 - val_accuracy: 0.9700\n",
      "Epoch 8/10\n",
      "450/450 [==============================] - 1s 1ms/step - loss: 0.0716 - accuracy: 0.9712 - val_loss: 0.0724 - val_accuracy: 0.9717\n",
      "Epoch 9/10\n",
      "450/450 [==============================] - 1s 1ms/step - loss: 0.0698 - accuracy: 0.9725 - val_loss: 0.0768 - val_accuracy: 0.9678\n",
      "Epoch 10/10\n",
      "450/450 [==============================] - 1s 1ms/step - loss: 0.0684 - accuracy: 0.9724 - val_loss: 0.0621 - val_accuracy: 0.9711\n",
      "\n",
      "Evaluation against test data:\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0771 - accuracy: 0.9640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0771091878414154, 0.9639999866485596]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training, validation, and testing\n",
    "history=model.fit(train_inputs,\n",
    "          train_labels,\n",
    "          batch_size = 16,\n",
    "          epochs = 10,\n",
    "          verbose = 1,\n",
    "          validation_split = 0.2)\n",
    "\n",
    "print(\"Evaluation against test data:\")\n",
    "model.evaluate(test_inputs, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "372a05cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: OUID_3C_Basic/OUID_3C_Basic\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: OUID_3C_Basic/OUID_3C_Basic\\assets\n"
     ]
    }
   ],
   "source": [
    "# Export the model and the scaler used with the data\n",
    "# Both will be stored in a directory matching the model name\n",
    "model.save(\"OUID_3C_Basic/OUID_3C_Basic\")\n",
    "dump(scaler, open('OUID_3C_Basic/scaler.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34ebf665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step\n",
      "Number of classification errors: 241\n",
      "Accuracy: 0.9759\n",
      "Confusion matrix:\n",
      "[[3432.   11.  128.]\n",
      " [  41. 1470.   12.]\n",
      " [   0.   49. 4857.]]\n"
     ]
    }
   ],
   "source": [
    "# For testing existing models on new data\n",
    "model = keras.models.load_model(\"../Finished Neural Network Models/OUID_3C_Optimized/OUID_3C_Optimized\")\n",
    "scaler = load(open('../Finished Neural Network Models/OUID_3C_Optimized/scaler.pkl','rb'))\n",
    "dataset = pd.read_csv(\"../Datasets/Three Coefficient 2nd Order/Dataset 2.csv\")\n",
    "labels = get_encoded_labels_overshoot_undershoot_classification(dataset)\n",
    "data_array = dataset.to_numpy()\n",
    "scaled_input = scaler.transform(data_array[:,0:3])\n",
    "raw_predictions = model.predict(scaled_input)\n",
    "predictions = np.argmax(raw_predictions,1)\n",
    "actual = np.argmax(labels,1)\n",
    "# Analysis of the performance of the model\n",
    "incorrect_indices = (predictions - actual).nonzero()\n",
    "num_incorrect = incorrect_indices[0].shape[0]\n",
    "print(\"Number of classification errors: \" + str(num_incorrect))\n",
    "print(\"Accuracy: \" + str((dataset.shape[0] - num_incorrect)/dataset.shape[0]))\n",
    "# Generate confusion matrix; rows correspond to actual label and columns correspond to predicted label\n",
    "confusion_matrix = np.zeros((3,3))\n",
    "for i in range(dataset.shape[0]):\n",
    "    if actual[i] == 0 and predictions[i] == 0:\n",
    "        confusion_matrix[0,0] = confusion_matrix[0,0] + 1\n",
    "    elif actual[i] == 1 and predictions[i] == 0:\n",
    "        confusion_matrix[1,0] = confusion_matrix[1,0] + 1\n",
    "    elif actual[i] == 2 and predictions[i] == 0:\n",
    "        confusion_matrix[2,0] = confusion_matrix[2,0] + 1\n",
    "    elif actual[i] == 0 and predictions[i] == 1:\n",
    "        confusion_matrix[0,1] = confusion_matrix[0,1] + 1\n",
    "    elif actual[i] == 1 and predictions[i] == 1:\n",
    "        confusion_matrix[1,1] = confusion_matrix[1,1] + 1\n",
    "    elif actual[i] == 2 and predictions[i] == 1:\n",
    "        confusion_matrix[2,1] = confusion_matrix[2,1] + 1\n",
    "    elif actual[i] == 0 and predictions[i] == 2:\n",
    "        confusion_matrix[0,2] = confusion_matrix[0,2] + 1\n",
    "    elif actual[i] == 1 and predictions[i] == 2:\n",
    "        confusion_matrix[1,2] = confusion_matrix[1,2] + 1\n",
    "    elif actual[i] == 2 and predictions[i] == 2:\n",
    "        confusion_matrix[2,2] = confusion_matrix[2,2] + 1\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c91047aa",
   "metadata": {},
   "source": [
    "Optimizing the hyperparameters (Run cells 1 through 5 before running these to prepare the dataset that will be used for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5560ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing different options for number of hidden layers and nodes per layer (keeping the nodes per layer uniform)\n",
    "# Here we generate the model for each combination\n",
    "num_layer_options = [2, 3, 4]\n",
    "nodes_per_layer_options = [64, 128, 256]\n",
    "indx_1 = indx_2 = 0\n",
    "# May add in a graph to compare training performance later\n",
    "#accuracy_measures = [[0 for j in range(len(num_layer_options))] for i in range(len(nodes_per_layer_options))]\n",
    "models = [[0 for j in range(len(num_layer_options))] for i in range(len(nodes_per_layer_options))]\n",
    "for nlo in num_layer_options:\n",
    "    for nplo in nodes_per_layer_options:\n",
    "        \n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.Dense(nplo,\n",
    "                                        input_shape=(3,),\n",
    "                                        name='1',\n",
    "                                        activation='relu'))\n",
    "        for i in range(nlo-1):\n",
    "            model.add(keras.layers.Dense(nplo,\n",
    "                                        name=str(i+2),\n",
    "                                        activation='relu'))\n",
    "        model.add(keras.layers.Dense(3,\n",
    "                                    name='output_layer',\n",
    "                                    activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "        model.fit(train_inputs,train_labels,batch_size = 16,epochs = 10,verbose = 0,validation_split = 0.2)\n",
    "        #accuracy_measures[indx_1][indx_2] = model.fit(train_inputs,train_labels,batch_size = 16,epochs = 10,verbose = 0,validation_split = 0.2).history[\"accuracy\"]\n",
    "        models[indx_1][indx_2] = model\n",
    "        indx_2 = indx_2 + 1\n",
    "    indx_1 = indx_1 + 1\n",
    "    indx_2 = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdd5b83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation against test data:\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0632 - accuracy: 0.9755\n",
      "Evaluation against test data:\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0676 - accuracy: 0.9696\n",
      "Evaluation against test data:\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 0.0647 - accuracy: 0.9724\n",
      "Evaluation against test data:\n",
      "3125/3125 [==============================] - 3s 983us/step - loss: 0.0991 - accuracy: 0.9614\n",
      "Evaluation against test data:\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0897 - accuracy: 0.9639\n",
      "Evaluation against test data:\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 0.0721 - accuracy: 0.9732\n",
      "Evaluation against test data:\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0918 - accuracy: 0.9691\n",
      "Evaluation against test data:\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0819 - accuracy: 0.9688\n",
      "Evaluation against test data:\n",
      "3125/3125 [==============================] - 5s 1ms/step - loss: 0.1377 - accuracy: 0.9441\n"
     ]
    }
   ],
   "source": [
    "# Load a separate dataset for testing\n",
    "test_dataset = pd.read_csv(\"../Datasets/Three Coefficient 2nd Order/Balanced Dataset 2.csv\")\n",
    "labels = get_encoded_labels_overshoot_undershoot_classification(test_dataset)\n",
    "data_array = test_dataset.to_numpy()\n",
    "scaled_input = scaler.transform(data_array[:,0:3])\n",
    "\n",
    "for i in range(len(num_layer_options)):\n",
    "    for j in range(len(nodes_per_layer_options)):\n",
    "        #Evaluate the model against the test dataset and print results\n",
    "        print(\"Evaluation against test data:\")\n",
    "        models[i][j].evaluate(scaled_input, labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fe31683",
   "metadata": {},
   "source": [
    "The best performance in terms of accuracy came from using 4 layers and 128 nodes per layer in my testing; note that the performance will vary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 14:38:14) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "a64d606a253e2d1816fece95c0230ff125080c831115324729bcd0cf449c2c57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
